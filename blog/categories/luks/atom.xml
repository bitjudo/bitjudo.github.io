<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: luks | bitJudo]]></title>
  <link href="http://bitjudo.com/blog/categories/luks/atom.xml" rel="self"/>
  <link href="http://bitjudo.com/"/>
  <updated>2014-06-09T20:42:21-04:00</updated>
  <id>http://bitjudo.com/</id>
  <author>
    <name><![CDATA[a couple of geeks]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Encrypting AWS Instance Storage]]></title>
    <link href="http://bitjudo.com/blog/2014/06/09/encrypting-aws-instance-storage/"/>
    <updated>2014-06-09T20:43:52-04:00</updated>
    <id>http://bitjudo.com/blog/2014/06/09/encrypting-aws-instance-storage</id>
    <content type="html"><![CDATA[<p><strong>Encrypted Data at Rest</strong> is the big term that has been floating around for several years. Just recently AWS started offering encrypted EBS volumes, the only problem with that is you cannot encrypt Instance Storage (aka Ephemeral Storage) volumes or Root volumes. This solution will not work for Root volumes, but it will for the Ephemeral volumes. The only potential problem with the encrypted EBS volumes is that AWS retains controls of the encryption keys for you in their IAM system. However since you&rsquo;ve chosen to use the cloud that might not be a problem. Thankfully using SaltStack and my previous trick <a href="">Just in Time Encryption Keys using SaltStack</a>, you can automatically encrypt your Instance Storage on your EC2 Instances giving you that extra layer of security. This can be extremely beneficial if you want to convey to your clients that you are doing <strong>Full Disk Encryption</strong> and you want the ability to use SSD storage instead of EBS volumes.</p>

<!-- more -->


<h2>Overview</h2>

<p>Using the same principles from my previous post about using SaltStack to deliver the encryption keys you can automatically encrypt all instance storage very quickly and effectively using LUKS. You will need SaltStack installed, with the <a href="https://github.com/saltstack/salt-contrib/blob/master/grains/ec2_tags.py">EC2 Grains plugin</a> (Please note that you&rsquo;ll need the Boto python plugin installed for it to work). The EC2 Grains plugin gathers all the metadata from about your instance and updates the minion&rsquo;s grain information. Using the EC2 grains we can find all attached ephemeral (aka instance storage) disks and encrypt them at boot.</p>

<p>Current this state has only been tested on Ubuntu systems, but should be easily expandable to others as well, just need to make sure the proper LVM packages are installed.</p>

<p>This state also stores the encrypt volume paths in a grain, so that you can access them from other states in Salt.</p>

<h2>The Breakdown</h2>

<p><strong>Lines 1 and 2</strong> are basic setup for the state, if you want to set a custom password, just make sure you set a <code>instanceluks:password</code> pillar data. You can also do per volume or drive passwords, on <strong>Line 25</strong></p>

<p><strong>Lines 4 through 16</strong> install the necessary crypsetup and LVM for ubuntu, with a little bit of work other operating systems can be supported.</p>

<p><strong>Line 18</strong> starts the loop through the instance storage (ephemeral disk)</p>

<p><strong>Line 19</strong> checks to see if the ephemeral disk exists, <strong>Note:</strong> Earlier I stated you had to install the ec2_tags.py grains module, this is why.</p>

<p><strong>Line 25</strong> if you set <code>instanceluks:passwords:NAME</code> in your pillar data, the state will use it, else it will use the global password set on <strong>Line 2</strong>.</p>

<p>The rest of the state are all the state commands to check if the LUKS partition has already been created, if not then create it, otherwise mount it after unlocking the volume.</p>

<p>After each volume is encrypted and mounted the state stores the volume mount path back into the hosts grains so that other states can access it, see <strong>Lines 60-63</strong></p>

<h2>The Full State</h2>

<p><div><script src='https://gist.github.com/8f88244ba6f8c253cfca.js?file=instanceluks.state'></script>
<noscript><pre><code>{%- set numbers = ['0', '1', '2', '3', '4', '5', '6', '7', '8'] -%}
{%- set global_password = salt['pillar.get']('instanceluks:password', 'testing12345') -%}

{% if grains['os_family'] == 'Debian' %}
instanceluks_crypto_package:
  pkg:
    - name: cryptsetup
    - order: 5
    - installed

instanceluks_lvm2_package:
  pkg:
    - name: lvm2
    - order: 6
    - installed
{% endif %}

{% for num in numbers %}
  {%- set device = salt['grains.get']('ec2_block-device-mapping_ephemeral' + num, false) -%}
  {% if device %}
    {%- if grains['os_family'] == 'Debian' -%}
    {%- set device = device|replace(&quot;sd&quot;, &quot;xvd&quot;) -%}
    {%- endif -%}
    {%- set name   = &quot;data&quot; + num -%}
    {%- set password = salt['pillar.get']('instanceluks:passwords:' + name, global_password) -%}

instanceluks_umount_{{ num }}:
  cmd.run:
    - name: umount /dev/{{ device }}
    - onlyif: df | grep /dev/{{ device }}
    - order: 10

instanceluks_encrypt_{{ num }}:
  cmd.run:
    - unless: cryptsetup luksUUID /dev/{{ device }}
    - name: echo &quot;{{ password }}&quot; | cryptsetup luksFormat /dev/{{ device }}
    - order: 12

instanceluks_open_{{ num }}:
  cmd.run:
    - unless: stat /dev/mapper/{{ name }}
    - name: echo &quot;{{ password }}&quot; | cryptsetup luksOpen /dev/{{ device }} {{ name }}
    - order: 14

instanceluks_format_{{ num }}:
  cmd.run:
    - unless: lsblk -f /dev/mapper/{{ name }} | grep ext4
    - name: mkfs.ext4 /dev/mapper/{{ name }}
    - order: 16

instanceluks_mount_{{ num }}:
  mount.mounted:
    - name: /{{ name }}
    - device: /dev/mapper/{{ name }}
    - fstype: ext4
    - mkmnt: True
    - persist: False
    - order: 18

instanceluks_grain_{{ num }}:
  grains.list_present:
    - name: instanceluks_volumes
    - value: /{{ name }}

  {% endif %}
{% endfor %}</code></pre></noscript></div>
</p>

<h2>Conclusion</h2>

<p>If you find this useful let me know. As always comments, questions, critique, etc is always welcome. Cheers!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Just in Time Encryption Keys using SaltStack]]></title>
    <link href="http://bitjudo.com/blog/2014/03/19/just-in-time-encryption-keys-using-saltstack/"/>
    <updated>2014-03-19T11:15:00-04:00</updated>
    <id>http://bitjudo.com/blog/2014/03/19/just-in-time-encryption-keys-using-saltstack</id>
    <content type="html"><![CDATA[<p>Recently, I was challenged with ensuring the encryption of all data at rest for several servers.  Unlike laptops or desktops, server nodes need to be able to come up and down in response to various requests. When spinning up multiple nodes you definitely don’t want them waiting for human interaction. Enter SaltStack and LUKS volumes. The real challenge was how to provide full disk encryption without storing the encryption key itself on the server.</p>

<p>Since these were Linux servers, LUKS encryption made the most sense. In essence what this tutorial describes is a way to provide “just in time” delivery of disk encryption keys. This is done using SaltStack features.</p>

<p>The rest of this article is a TL;DR combined with a tutorial of sorts to help you set this up.</p>

<h2>TL;DR</h2>

<p>By taking advantage of a couple features that <a href="http://www.saltstack.com/">SaltStack</a> brings to the table, it is possible to automate the mounting of your LUKS volumes after the server has started. The salt minion has the ability to run certain states (scripts) upon start.  This allows the user to run a LUKS state that will verify the existence of the volume, unlock it, and mount it. Using salt states also allows the user to build state dependencies, or trigger other states to run.  These features ensure that any services requiring the encrypted volume only start after the volume is available.</p>

<!-- more -->


<h2>How it Works</h2>

<p>Upon initializing of SaltStack, the configuration tells the Salt minion to run two scripts.  One script is the LUKS encryption script, which confirms that the LUKS volume exists, then unlocks and mounts it.  When the LUKS script runs, it requests pillar data from the Salt master.  Included in the requested data is the LUKS encryption key, which is transferred using zeromq and AES encryption. The key is temporarily stored locally on the minion while the script runs, and is deleted upon completion.</p>

<p>The second script checks whether a particular service is dependent upon the encrypted volume is running.  If the specific service is found to not be running, the script will also initialize it.  In this particular case, the service is a mongodb service.</p>

<p>Ultimately, we end up with the ability to mount an encrypted drive and start dependent services.  However, we have relegated the encryption key to a single separate server and only temporarily transfer it to the servers that need it, exactly when they need it.  In simpler terms, we’ve created a just-in-time delivery mechanism for an encryption key.  While this method may not be bulletproof, I have found it preferable to storing the encryption key permanently on the same server as the paired encrypted drive.</p>

<h2>The Quasi How-To</h2>

<p>The below does not cover setup and/or configuring SaltStack, LVM or LUKS encryption.  Rather, it’s an example of salt state and pillar files, some steps you can use to get going. It should be pretty straightforward.</p>

<h3>The Salt State</h3>

<p>I originally found this state and after a little bit of searching I cannot find the original place I found it, but I ended up modifying it a bit for my needs. If you know where it originally came from, let me know and I’ll make a note. The state ensures that the LVM I need is setup and then sets up the LUKS volume. If all that is already setup then the script will simply unlock and mount the LUKS volume at the the defined mount point.</p>

<p><em>Assumptions:</em> You have a non-formatted drive attached to the server and you&rsquo;ve defined that in your pillar data, make sure you assign the correct drive device, otherwise you might end up erasing data.</p>

<p><div><script src='https://gist.github.com/9541650.js?file=luks.state'></script>
<noscript><pre><code>{% set password = pillar['encryption']['password'] %}
{% set devname = pillar['encryption']['dev_name'] %}
{% set volgroup = pillar['encryption']['vg_name'] %}
{% set mountpoint = pillar['encryption']['mountpoint'] %}

crypto-package:
  pkg:
    - name: cryptsetup
    - order: 10
    - installed

lvm2-package:
  pkg:
    - name: lvm2
    - order: 11
    - installed

{{ devname }}:
  lvm.pv_present:
    - order: 12

vg{{ volgroup }}:
  lvm.vg_present:
    - devices: {{ devname }}
    - order: 13

lv{{ mountpoint }}:
  cmd.run:
    - unless: lvdisplay /dev/vg{{ volgroup}}/lv{{ volgroup }}
    - name: lvcreate -l 100%FREE -n lv{{ volgroup }} vg{{ volgroup }}
    - order: 14

enc_volume:
  cmd.run:
    - unless: cryptsetup luksUUID /dev/vg{{ volgroup }}/lv{{ mountpoint }}
    - name: echo &quot;{{ password }}&quot; | cryptsetup luksFormat /dev/vg{{ volgroup }}/lv{{ mountpoint }}
    - order: 15

enc_volume_open:
  cmd.run:
    - unless: stat /dev/mapper/{{ mountpoint }}
    - name: echo &quot;{{ password }}&quot; | cryptsetup luksOpen /dev/vg{{ volgroup }}/lv{{ mountpoint }} {{ mountpoint }}
    - order: 16

enc_volume_format:
  cmd.run:
    - unless: lsblk -f /dev/mapper/{{ mountpoint }} | grep ext4
    - name: mkfs.ext4 /dev/mapper/{{ mountpoint }}
    - order: 17

enc_volume_mount:
  mount.mounted:
    - name: /{{ mountpoint }}
    - device: /dev/mapper/{{ mountpoint }}
    - fstype: ext4
    - mkmnt: True
    - opts: noatime,nodiratime
    - persist: False
    - order: 18
</code></pre></noscript></div>
</p>

<p><strong>Note:</strong> Please note that this state will create the LVM physical and logical volumes or attempt too if that fails LUKS and mounting will fail too.</p>

<p><strong>Caveat:</strong> This script will initialize a single drive as part of an LVM, with some tweaking it could combine multiple drives. This script was design to bring a server up from scratch and have a LUKS volume mounted and ready to go.</p>

<h4>Extra Credit</h4>

<p>If you want to learn how to make states reliant on each other check out the <code>requires</code> directive in the salt documentation. If you want to notify states when another state is done running check out <code>require_in</code>.</p>

<h3>The Salt Pillar</h3>

<p>The pillar contains the configuration for the LVM and LUKS volumes and the mount point. Ensuring that you populate this pillar with the appropriate data is important.</p>

<ul>
<li><strong>password</strong> is the encryption key for the LUKS volume.</li>
<li><strong>vg_name</strong> is used to define both the volume group but the logical volume name too.</li>
<li><strong>mountpoint</strong> is where you want the LVM and LUKS volume to be mounted.</li>
<li><strong>dev_name</strong> is the device that you want converted to LVM and then LUKS encrypted.</li>
</ul>


<p><div><script src='https://gist.github.com/9541650.js?file=luks.pillar'></script>
<noscript><pre><code>encryption:
  password: some_random_long_string
  dev_name: /dev/sda1
  vg_name: data
  mountpoint: data</code></pre></noscript></div>
</p>

<p><strong>Note:</strong> I don&rsquo;t discuss how to do it, but you can duplicate this pillar and change the <strong>password</strong> for each server if you want.</p>

<h3>Putting it all together</h3>

<p>Assuming you&rsquo;ve associated the salt state and pillar to the appropriate target you&rsquo;ll be able to use <code>highstate</code> or <code>state.sls</code> to setup your LUKS encrypted drive. To make sure this happens each time the server starts up make sure you edit the minion&rsquo;s configuration and setup <code>startup_states</code> to <em>highstate</em>.</p>

<p>You should be able to tell the minion to run the LUKS state and find your drive mounted. Once that happens do a quick reboot, you should also find a few seconds to a minute after reboot your drive is mounted and accessible.</p>

<h2>Conclusion</h2>

<p>I&rsquo;m pretty happy with this solution, it allows my servers to be rebooted without human interaction and for the critical services to come online once their data is available. It keeps the encryption key from having to be stored on the server. Using this strategy you could easily expand this for other encryption technologies.</p>

<p>Thoughts, comments, questions? Let me hear from you. Leave your comments below.</p>
]]></content>
  </entry>
  
</feed>
